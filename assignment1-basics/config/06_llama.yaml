project_name: "cs336-exp6-arch-llama"

common:
  data:
    train_path: "output/owt_train.npy"
    val_path: "output/owt_valid.npy"
  training:
    device: "cuda"
    batch_size: 64
    max_iters: 25000  # 适当调整步数
    learning_rate: 3e-4
    min_lr: 3e-5
    warmup_iters: 1000
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
    eps: 1e-8
    grad_clip: 1.0
  checkpoint:
    out_dir_base: "checkpoints/exp1"
    save_interval: 2000

experiments:
  # Llama 架构
  - run_name: "Llama-Architecture"
    model:
      vocab_size: 10000
      context_length: 256
      d_model: 256
      num_layers: 4
      num_heads: 4
      d_ff: 688            # SwiGLU 维度 (256 * 8/3 ≈ 682 -> 688)
      rope_theta: 10000.0
      pos_emb_type: "rope"
      norm_location: "pre"
      norm_type: "rmsnorm"
      ffn_type: "swiglu"
      weight_tying: true
      dropout: 0.1