### Linear模块

#### 初始化参数：

在神经网络的参数初始化中，截断正态分布主要用于**解决数值稳定性问题，防止梯度消失或爆炸**，从而加快模型的收敛速度 。

- **避免极端值**：普通的正态分布在尾部（离均值很远的地方）仍有概率取到非常大或非常小的值。如果初始权重过大，经过多层网络后，激活值可能会变得极大（导致梯度爆炸或在某些激活函数上饱和）；如果过小，则可能导致信号衰减（梯度消失）。
- **截断的作用**：通过限制取值范围（例如限制在均值左右的 2 或 3 个标准差内），可以保证所有的初始权重都在一个合理的、受控的范围内，让网络在训练初期更加稳定。文档提到，虽然 Pre-norm Transformer 对初始化相对鲁棒，但好的初始化依然能显著提升训练表现 。

#### 初始化公式：

**均值 ($\mu$)**：`0`

**方差 ($\sigma^2$)**：$\frac{2}{d_{in} + d_{out}}$

**标准差 (`std`)**：$\sqrt{\frac{2}{d_{in} + d_{out}}}$

**截断范围**：$\pm 3$ 倍标准差。调用 `trunc_normal_` 并传入计算出的 `std` 以及截断边界（`a` 和 `b` 参数）。

#### 权重形状

许多机器学习论文在符号中使用**行向量**，这种表示方式能很好地契合 NumPy 和 PyTorch 默认使用的**行优先**内存排序。在使用行向量时，线性变换的形式如下：
$$
y = xW^T \quad
$$
其中 $W \in \mathbb{R}^{d_{out} \times d_{in}}$ 为行优先矩阵，而 $x \in \mathbb{R}^{1 \times d_{in}}$ 为行向量。

在数学的线性代数中，通常更习惯使用**列向量**，此时线性变换的形式如下：
$$
y = Wx \quad 
$$
其中 $W \in \mathbb{R}^{d_{out} \times d_{in}}$ 为行优先矩阵，而 $x \in \mathbb{R}^{d_{in}}$ 为列向量。**在本作业中，我们的数学符号将使用列向量**，但请记住，如果想使用普通的矩阵乘法符号（在代码中实现），必须按照行向量的惯例进行矩阵运算，因为 PyTorch 使用的是行优先的内存排序。如果你使用 `einsum` 来进行矩阵操作，那么这就不是问题了。

#### 实现

实现一个方程：$$y = xW^T$$

##### $\textcolor{red}{消融实验1}$

**注意，现代LLM不包含偏置**（作为消融实验1）



### Embedding模块、

#### 原理

实现一个嵌入层 (Embedding Layer)，用于将 token ID 映射为密集向量。

输入向量：(batch_size, sequence_length)

输出向量：(batch_size, sequence_length, embedding_dim（d_model）)

权重向量：(num_embeddings,（vocab） embedding_dim(d_model))

$$(B, T) \xrightarrow{\text{Lookup}} (B, T, D)$$

这个就是根据某一个词表进行查询，然后完成



#### look up的实现方式：

One-hot 向量里只有一个 `1`，其他都是 `0`，那矩阵乘法的结果其实就是**把权重矩阵里对应的那一行“复制”出来**。

**self.weight[token_ids]**

- 输入 `token_ids` 形状：`(B, T)`
- 权重 `weight` 形状：`(V, D)`
- 对于 `token_ids` 里的每一个元素 `id`，PyTorch 都会用 `weight[id, :]`（形状为 `(D,)` 的向量）来替换它。
- 其实就是对于每一个id，取行作为下标，然后恢复成原来的形状。
- **最终形状**：`(B, T)` $\rightarrow$ `(B, T, D)`