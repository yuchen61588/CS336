from __future__ import annotations

import os
import json
import regex as re
import heapq
import multiprocessing
import tqdm
from functools import partial
from typing import List, Dict, Tuple, Optional, Iterable, Iterator, Union
import time
from cs336_basics.train_bpe import _get_chunk_boundaries

GPT2_SPLIT_PATTERN = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
# -----------------------------------------------------------------------------
# å¹¶è¡ŒåŒ– Worker å…¨å±€å˜é‡ä¸å‡½æ•° (å¿…é¡»å®šä¹‰åœ¨ç±»å¤–éƒ¨ä»¥æ”¯æŒ Pickle)
# -----------------------------------------------------------------------------
_worker_tokenizer = None


def _worker_initializer(serialized_vocab: Dict[int, List[int]],
                        merges_list: List[Tuple[bytes, bytes]],
                        special_tokens: List[str]):
    """
    å­è¿›ç¨‹åˆå§‹åŒ–å‡½æ•°ï¼šåœ¨æ¯ä¸ª Worker è¿›ç¨‹å¯åŠ¨æ—¶é‡å»º Tokenizer å®ä¾‹ã€‚
    é¿å…æ¯å¤„ç†ä¸€ä¸ª Chunk å°±ä¼ é€’ä¸€æ¬¡å·¨å¤§çš„ Tokenizer å¯¹è±¡ã€‚
    """
    global _worker_tokenizer
    # é‡å»º Vocab (List[int] -> bytes)
    vocab = serialized_vocab
    _worker_tokenizer = Tokenizer(100000, vocab, merges_list, special_tokens)


# =============================================================================
# æ–°å¢ï¼šä¸“é—¨å¤„ç†æ–‡ä»¶çš„ Worker
# =============================================================================
def _worker_encode_from_file(args) -> Tuple[List[int], int]:
    """
    Worker å˜ä½“ï¼šæ¥æ”¶ (æ–‡ä»¶è·¯å¾„, start, end)ï¼Œè¯»å–æ–‡æœ¬åè°ƒç”¨åŸæœ‰çš„ encodeã€‚
    """
    path, start, end = args

    # 1. ä½¿ç”¨ seek + read è¯»å–æŒ‡å®šç‰‡æ®µ (åˆ©ç”¨ OS Cacheï¼Œé€Ÿåº¦æå¿«)
    # è¿™é‡Œä¸éœ€è¦å†ç”¨ mmap å¯¹è±¡ï¼Œç›´æ¥æ–‡ä»¶ IO å³å¯ï¼Œå› ä¸ºè¾¹ç•Œå·²ç»ç®—å¥½äº†
    with open(path, 'rb') as f:
        f.seek(start)
        chunk_bytes = f.read(end - start)

    # 2. å¤„ç†æ¢è¡Œç¬¦å’Œè§£ç 
    if b'\r\n' in chunk_bytes:
        chunk_bytes = chunk_bytes.replace(b'\r\n', b'\n')

    # decode ä¼šå¾—åˆ°åŸæœ¬çš„å¤§æ®µæ–‡æœ¬
    text_chunk = chunk_bytes.decode('utf-8', errors='replace')

    # encode å†…éƒ¨ä¼šåšï¼šSpecial Tokenéš”ç¦» -> æ­£åˆ™åˆ‡åˆ† -> BPEåˆå¹¶
    ids = _worker_tokenizer.encode(text_chunk)

    # è¿”å› ids å’Œ å­—èŠ‚æ•°(ç”¨äºè¿›åº¦æ¡)
    return ids, len(chunk_bytes)

    return _worker_tokenizer.encode(text)


def _worker_encode(text: str) -> Tuple[List[int], int]:
    """å¤„ç†å†…å­˜å­—ç¬¦ä¸²çš„ Worker"""
    # è¿™é‡Œçš„ _worker_tokenizer æ˜¯å…¨å±€å˜é‡
    ids = _worker_tokenizer.encode(text)
    # è¿”å› (ids, å­—èŠ‚é•¿åº¦)
    return ids, len(text.encode('utf-8'))


class Tokenizer:
    def __init__(self, max_cache: int, vocab: Dict[int, bytes], merge: List[Tuple[bytes, bytes]],
                 special_tokens: List[str] = None, ):
        """
                åˆå§‹åŒ–åˆ†è¯å™¨ã€‚
                Args:
                    vocab: Token ID åˆ° bytes çš„æ˜ å°„ã€‚
                    merges: åˆå¹¶è§„åˆ™åˆ—è¡¨ï¼Œé¡ºåºä»£è¡¨ä¼˜å…ˆçº§ã€‚
                    special_tokens: ç‰¹æ®Š Token åˆ—è¡¨ (å¦‚ <|endoftext|>)ã€‚
                """

        self.vocab = {int(k): v for k, v in vocab.items()}  # id->byte
        self.vocab_inv = {v: k for k, v in vocab.items()}  # byte->id

        # å°† merges åˆ—è¡¨è½¬æ¢ä¸º ranks å­—å…¸ï¼Œå®ç° O(1) æŸ¥æ‰¾ä¼˜å…ˆçº§
        # Key: (byte_token_1, byte_token_2), Value: Rank (è¶Šå°è¶Šä¼˜å…ˆ)
        # self.ranks = {pair : i for i, pair in enumerate(merge)}
        self.ranks: Dict[Tuple[int, int], int] = {}
        self.merge_map: Dict[Tuple[int, int], int] = {}
        self.original_merges = merge

        for rank, (p1, p2) in enumerate(merge):
            # åªæœ‰å½“ä¸¤ä¸ª token éƒ½åœ¨è¯è¡¨ä¸­æ—¶æ‰æ·»åŠ è§„åˆ™
            if p1 in self.vocab_inv and p2 in self.vocab_inv:
                id1 = self.vocab_inv[p1]  # è¿™é‡Œç°åœ¨ä¿è¯æ˜¯ int
                id2 = self.vocab_inv[p2]
                pair = (id1, id2)
                self.ranks[pair] = rank

                # é¢„è®¡ç®—åˆå¹¶åçš„ ID
                merged_bytes = p1 + p2
                if merged_bytes in self.vocab_inv:
                    self.merge_map[pair] = self.vocab_inv[merged_bytes]
        self.special_tokens = special_tokens if special_tokens else []
        # é¢„ç¼–è¯‘æ­£åˆ™
        self.gpt2_pat = re.compile(GPT2_SPLIT_PATTERN)
        # æ„é€ ç‰¹æ®Š Token æ­£åˆ™ï¼Œç”¨äºåœ¨é¢„åˆ†è¯å‰è¿›è¡Œç‰©ç†éš”ç¦»
        # æ³¨æ„ï¼Œå¯¹äºä¸¤ä¸ªè¿ç»­çš„ï¼Œè¦ä¼˜å…ˆåŒ¹é…é•¿çš„æ­£åˆ™
        self.special_pat = None
        if self.special_tokens:
            # å¿…é¡»ä½¿ç”¨ re.escape ç¡®ä¿ç‰¹æ®Šå­—ç¬¦è¢«å½“ä½œå­—é¢é‡
            # ä½¿ç”¨æ•è·ç»„ () ç¡®ä¿ split åä¿ç•™åˆ†éš”ç¬¦
            # æ’åºï¼šä¼˜å…ˆåŒ¹é…è¾ƒé•¿çš„ç‰¹æ®Š Tokenï¼Œé˜²æ­¢ "<|endoftext|>" è¢« "<|end|>" æˆªæ–­
            # æ˜¯ä»å·¦åˆ°å³å°è¯•åŒ¹é…çš„ï¼Œè€Œä¸”ä¸€æ—¦åŒ¹é…æˆåŠŸå°±ç«‹å³åœæ­¢ï¼ˆEager Matchingï¼‰ã€‚å®ƒä¸ä¼šè‡ªåŠ¨å¯»æ‰¾â€œæœ€é•¿â€çš„åŒ¹é…ï¼Œè€Œæ˜¯å¯»æ‰¾â€œæœ€æ—©â€çš„åŒ¹é…ã€‚
            sorted_specials = sorted(self.special_tokens, key=len, reverse=True)
            pattern_str = "|".join(re.escape(st) for st in sorted_specials)
            self.special_pat = re.compile(f"({pattern_str})")
        else:
            special_tokens = None

            # ç¼“å­˜: å°† åŸå§‹bytesç‰‡æ®µ æ˜ å°„ä¸º Token ID åˆ—è¡¨
            # å¯¹åº” markdown ä¸­çš„ cache ä¼˜åŒ–
        self.cache: Dict[bytes, List[int]] = {}
        self.MAX_CACHE = max_cache

    @classmethod
    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: List[str] = None) -> Tokenizer:
        """
        ä»æ–‡ä»¶åŠ è½½è¯è¡¨å’Œåˆå¹¶è§„åˆ™ã€‚
        """
        with open(vocab_filepath, 'r', encoding='utf-8') as f:
            # json load å‡ºæ¥çš„ key é»˜è®¤æ˜¯ strï¼Œéœ€è¦è½¬ int
            raw_vocab = json.load(f)
            # å…¼å®¹ï¼šæœ‰äº›ä¿å­˜æ˜¯å°† bytes å­˜ä¸º unicode string (latin-1) æˆ– 16è¿›åˆ¶
            # è¿™é‡Œå‡è®¾ä¿å­˜æ ¼å¼ç¬¦åˆ assignment è¦æ±‚ï¼Œvalue æ˜¯ bytes å¯¹åº”çš„ string è¡¨è¾¾
            # å¦‚æœæ˜¯ train_bpe.py ç”Ÿæˆçš„ï¼Œé€šå¸¸éœ€è¦ç¡®ä¿å­˜å‚¨æ ¼å¼æ­£ç¡®ã€‚
            # ä¸ºäº†é€šç”¨æ€§ï¼Œè¿™é‡Œå‡è®¾ json å­˜çš„æ˜¯ {id: "str_repr"}ï¼Œéœ€è¦ encode å› bytes
            # æˆ–è€…å¦‚æœæ˜¯ pickle/pt åˆ™ç›´æ¥è¯»å–ã€‚è¿™é‡Œéµå¾ªæ ‡å‡† JSON æ–‡æœ¬æ ¼å¼ã€‚
            # è¿™é‡Œçš„è½¬æ¢é€»è¾‘å–å†³äº save çš„æ–¹å¼ï¼Œå‡è®¾ save æ—¶æ˜¯ç”¨ latin-1 è§£ç ä¿å­˜çš„

        vocab = {}
        for k, v in raw_vocab.items():
            # JSON ä¸æ”¯æŒ bytesï¼Œé€šå¸¸å­˜ä¸º latin1 å­—ç¬¦ä¸²
            if isinstance(v, list):
                vocab[int(k)] = bytes(v)
            elif isinstance(v, str):
                vocab[int(k)] = v.encode('latin-1')
            else:
                vocab[int(k)] = v  # trainç”¨çš„æ˜¯æ—§ç‰ˆå­˜å‚¨æ ¼å¼

        # åŠ è½½merge
        # merges.txt é€šå¸¸æ¯è¡Œæ˜¯ä¸€ä¸ª pairï¼Œä¾‹å¦‚ "u g"
        # æˆ–è€…å­˜ä¸º json list
        merges = []
        with open(merges_filepath, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                line = line.strip()
                if not line:
                    continue
                # å˜åŒ–ç‚¹ï¼šè§£æ "[...]" æ ¼å¼
                # å¯»æ‰¾ä¸­é—´çš„åˆ†éš”ç¬¦ "] [" æ¥åˆ‡åˆ† p0 å’Œ p1
                split_idx = line.find('] [')
                if split_idx == -1:
                    print(f"Warning: Line {line_num + 1} format error, skipping: {line}")
                    continue
                try:
                    # æå–å­—ç¬¦ä¸²éƒ¨åˆ†: "[32, 116]"
                    p1_str = line[:split_idx + 1]
                    p2_str = line[split_idx + 1:].strip()

                    # ä½¿ç”¨ json.loads å°†å­—ç¬¦ä¸² "[32, 116]" å˜å›åˆ—è¡¨ [32, 116]
                    p1_ids = json.loads(p1_str)
                    p2_ids = json.loads(p2_str)

                    # è½¬å› bytes å¯¹è±¡ï¼Œè¿™æ˜¯ Tokenizer å†…éƒ¨é€»è¾‘éœ€è¦çš„æ ¼å¼
                    merges.append((bytes(p1_ids), bytes(p2_ids)))
                except Exception as e:
                    print(f"Warning: Failed to parse line {line_num + 1}: {line} | Error: {e}")
                    continue

        return cls(vocab, merges, special_tokens)

    def _merge_tokens(self, token_ids: List[int]) -> List[int]:
        """
        ã€æ ¸å¿ƒä¼˜åŒ–ã€‘æ¨ç†é˜¶æ®µçš„åˆå¹¶é€»è¾‘ã€‚
        ä½¿ç”¨ æœ€å°å † (Heap) + åŒå‘é“¾è¡¨ (Linked List) ç­–ç•¥ã€‚
        å¤æ‚åº¦ï¼šO(N log N) è€Œé O(N^2)ã€‚
        """
        # å°äº2æ— æ³•åˆå¹¶
        if len(token_ids) < 2:
            return token_ids

        # åˆå§‹åŒ–åŒå‘é“¾è¡¨ç»“æ„
        # idä»0å¼€å§‹ï¼Œnextè¡¨ç¤ºä¸‹ä¸€ä¸ªæœ‰æ•ˆä¸‹æ ‡ï¼ŒprevæŒ‡å‘ä¸Šä¸€ä¸ªï¼Œæ²¡æœ‰å°±è¡¨ç¤º-1
        n = len(token_ids)
        next_pos = [i + 1 for i in range(n)]
        next_pos[-1] = -1
        prev_pos = [i - 1 for i in range(n)]

        # æ ‡è®°å½“å‰ä½ç½®çš„ token æ˜¯å¦æœ‰æ•ˆ (Lazy Delete éœ€è¦ç”¨åˆ°)
        # æˆ‘ä»¬ç›´æ¥ä¿®æ”¹ token_ids åˆ—è¡¨ï¼Œå¦‚æœåˆå¹¶äº†ï¼ŒåŸæ¥çš„ä½ç½®å°±ä¸è¯»äº†

        # 2. åˆå§‹åŒ–å †
        # å †å…ƒç´ : (rank, start_index)
        # start_index æ˜¯ pair å·¦ä¾§ token çš„ä¸‹æ ‡
        pq = []

        def get_rank(idx):
            """å°è¯•è·å– (tokens[idx], tokens[next_pos[idx]]) çš„ rank"""
            if idx == -1 or next_pos[idx] == -1:
                return None

            # ä» vocab è¿˜åŸ bytes æ‰èƒ½æŸ¥ rank
            # æ³¨æ„ï¼šranks é‡Œçš„ key æ˜¯ (bytes, bytes)
            # ä¼˜åŒ–ï¼šåœ¨åˆå§‹åŒ–æ—¶å¯ä»¥å°† ranks è½¬ä¸º (id, id) -> rankï¼Œé¿å… lookup vocab
            # ä½†ç”±äº assignment è¦æ±‚ vocab æ˜¯ bytesï¼Œè¿™é‡Œåšä¸€æ¬¡è½¬æ¢ï¼Œæˆ–è€…å‡è®¾ ranks å·²ç»åšäº†è½¬æ¢
            # ä¸ºäº†ä¸¥è°¨ï¼Œæˆ‘ä»¬æŸ¥ vocabã€‚ä¸ºäº†æ€§èƒ½ï¼Œå¯ä»¥åœ¨ __init__ é‡ŒæŠŠ ranks æ˜ å°„æˆ idã€‚
            # è¿™é‡Œéµå¾ªæ–‡æ¡£çš„ ranks å®šä¹‰ã€‚

            pair = (token_ids[idx], token_ids[next_pos[idx]])
            return self.ranks.get(pair)

        # åˆå§‹æ‰«æ æ„å»ºé¢‘ç‡å †
        for i in range(n - 1):
            rank = get_rank(i)
            if rank is not None:
                heapq.heappush(pq, (rank, i))

        # å¾ªç¯åˆå¹¶
        while pq:
            rank, i = heapq.heappop(pq)
            # --- æ‡’æƒ°åˆ é™¤æ ¡éªŒ (Lazy Check) ---
            # æ£€æŸ¥é“¾è¡¨è¿é€šæ€§ï¼šå¦‚æœ i çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„ä¸Šä¸€ä¸ªèŠ‚ç‚¹ä¸æ˜¯ iï¼Œè¯´æ˜ i å·²ç»è¢«è·³è¿‡æˆ–æ–­è£‚
            # ç”¨äºé¢‘ç‡å †çš„å‡åˆ é™¤ï¼Œå°±æ˜¯é€šè¿‡è·³è¿‡æ¥ä¸è®¿é—®åˆ é™¤çš„é¢‘ç‡å †
            if next_pos[i] == -1 or prev_pos[next_pos[i]] != i:
                continue
            # äºŒæ¬¡æ ¡éªŒ Rank (é˜²æ­¢åŒä¸€ä¸ªä½ç½®çš„ Pair è¢«å¤šæ¬¡æ¨å…¥å †ï¼Œrank å‘ç”Ÿäº†å˜åŒ–)
            # è™½ç„¶ BPE è¿™é‡Œçš„ Rank æ˜¯å›ºå®šçš„ï¼Œä½†ä¸ºäº†é€»è¾‘ä¸¥å¯†
            # æ£€éªŒé‚£ç§è™½ç„¶åˆå¹¶äº†ï¼Œä½†æ˜¯è¿˜ä¿æŒè¿æ¥çš„é‚£ç§ï¼ˆåˆå¹¶ä½†æ˜¯é‚»å±…å˜äº†ï¼‰
            current_rank = get_rank(i)
            if current_rank != rank:
                continue

            # æ‰§è¡Œåˆå¹¶
            j = next_pos[i]
            pair = (token_ids[i], token_ids[j])
            if pair in self.merge_map:
                new_token_id = self.merge_map[pair]
            else:
                # å¼‚å¸¸æƒ…å†µï¼šmerges é‡Œæœ‰ï¼Œä½† vocab é‡Œæ²¡æœ‰ (è®­ç»ƒæ•°æ®ä¸ä¸€è‡´)
                continue

            # æ›´æ–°å½“å‰ä½ç½®iä¸ºtoken è¿™é‡Œä¸ä½¿ç”¨è¡¨æ ¼çš„æ–¹æ³•ï¼Œä½¿ç”¨åŸåœ°ä¿®æ”¹æ³•,å‰é¢çš„ç´¢å¼•ä¸ç”¨åˆ 
            token_ids[i] = new_token_id

            # æ›´æ–°é“¾è¡¨æŒ‡é’ˆ
            k = next_pos[j]
            next_pos[i] = k
            if k != -1:
                # å·²ç»æ²¡æœ‰ä¸‹ä¸€ä¸ªäº† è¿™ä¸ªkå°±å·²ç»æ¯«æ— æ„ä¹‰äº†
                prev_pos[k] = i

            # æ£€æŸ¥æ–°äº§ç”Ÿçš„é‚»å±…
            # 1. æ£€æŸ¥å·¦é‚»å±…: (prev_pos[i], i)
            if prev_pos[i] != -1:
                left_neighbor = prev_pos[i]
                new_rank_left = get_rank(left_neighbor)
                if new_rank_left is not None:
                    heapq.heappush(pq, (new_rank_left, left_neighbor))

            # 2. æ£€æŸ¥å³é‚»å±…: (i, next_pos[i])
            # æ³¨æ„ï¼ši ç°åœ¨æ˜¯æ–° tokenï¼Œnext_pos[i] æ˜¯åŸæ¥çš„ k
            if next_pos[i] != -1:
                new_rank_right = get_rank(i)
                if new_rank_right is not None:
                    heapq.heappush(pq, (new_rank_right, i))

        # 4. é‡ç»„ç»“æœåˆ—è¡¨
        # ä»å¤´(0)æˆ–ç¬¬ä¸€ä¸ªæœ‰æ•ˆèŠ‚ç‚¹å¼€å§‹éå† next_pos
        result = []

        # å¯»æ‰¾å¤´èŠ‚ç‚¹ æ‰¾åˆ°prev[i] = -1çš„ 0æ°¸è¿œæ˜¯é“¾è¡¨å¤´
        cur = 0
        while cur != -1:
            result.append(token_ids[cur])
            cur = next_pos[cur]

        return result

    def encode(self, text: str) -> List[int]:
        """
        å¹¶è¡Œç¼–ç å¤šä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆé€‚ç”¨äºå¤„ç†æ•°æ®é›†ï¼Œå¦‚ OpenWebTextï¼‰ã€‚
        Args:
            texts: å­—ç¬¦ä¸²åˆ—è¡¨ (List[str])
            num_processes: è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä½¿ç”¨ CPU æ ¸å¿ƒæ•° - 1

        Returns:
            List[int]: æ‰€æœ‰æ–‡æœ¬å±•å¹³åçš„ Token ID åˆ—è¡¨
        """
        if not text:
            return []
            # 1. ç‰¹æ®Š Token åˆ‡åˆ†
        if self.special_pat:
            # ä½¿ç”¨ split åï¼Œä¿ç•™åˆ†éš”ç¬¦(åœ¨æ‹¬å·é‡Œ)ï¼Œæ‰€ä»¥åˆ—è¡¨é‡Œä¼šæœ‰ special tokens
            raw_segments = self.special_pat.split(text)
        else:
            raw_segments = [text]

        ids = []
        for segment in raw_segments:
            if not segment:  # Kç©ºçš„
                continue
            if segment in self.special_tokens:
                seg_bytes = segment.encode('utf-8')
                if seg_bytes in self.vocab_inv:
                    ids.append(self.vocab_inv[seg_bytes])
                continue  # é˜²æ­¢æ”¾åˆ°ä¸‹é¢

            pre_tokens = self.gpt2_pat.findall(segment)
            for token_str in pre_tokens:
                token_bytes = token_str.encode('utf-8')  # ä¸€ä¸ªè¯è¿›è¡Œåºåˆ—åŒ–

                # ç¼“å­˜
                if token_bytes in self.cache:
                    ids.extend(self.cache[token_bytes])
                    continue
                # 3. è½¬åˆå§‹ ID (Bytes -> Ints)
                # ä½¿ç”¨ vocab_inv å°†æ¯ä¸ªå­—èŠ‚æ˜ å°„ä¸ºåŸºç¡€ ID (0-255)
                current_ids = [self.vocab_inv[bytes([b])] for b in token_bytes]

                # bpeåˆå¹¶
                merged_ids = self._merge_tokens(current_ids)  # å·²ç»åˆå¹¶å®Œæˆäº†
                # æ›´æ–° Cache
                if len(self.cache) < self.MAX_CACHE:  # ç®€å•å®¹é‡é™åˆ¶
                    self.cache[token_bytes] = merged_ids

                ids.extend(merged_ids)

                # zè£…å…¥ç¼“å­˜
                if len(self.cache) < self.MAX_CACHE:
                    self.cache[token_bytes] = merged_ids

        return ids

    def decode(self, ids: List[int]) -> str:
        """è§£ç ï¼šIDs -> Bytes -> String (with replacement)"""
        byte_parts = []
        for i in ids:
            if i in self.vocab:
                byte_parts.append(self.vocab[i])
        all_bytes = b"".join(byte_parts)
        return all_bytes.decode('utf-8', errors='replace')

    # =========================================================================
    #  å¹¶è¡ŒåŒ–æ¥å£
    # =========================================================================

    def _chunk_string(self, text: str, num_chunks: int) -> List[str]:
        """
        æ¨¡ä»¿ train_bpe.py çš„é€»è¾‘ï¼Œå°†å†…å­˜ä¸­çš„å¤§å­—ç¬¦ä¸²åˆ‡åˆ†ä¸º num_chunks ä»½ã€‚
        å…³é”®ç‚¹ï¼š
        1. å¯»æ‰¾æœ€è¿‘çš„æ¢è¡Œç¬¦ '\n' ä½œä¸ºåˆ‡åˆ†ç‚¹ï¼Œé˜²æ­¢åˆ‡æ–­å•è¯ã€‚
        2. ç›¸æ¯” splitlines()ï¼Œå®ƒä¿ç•™äº†æ¢è¡Œç¬¦ï¼Œä¸”ä¸ä¼šäº§ç”Ÿæ•°ç™¾ä¸‡ä¸ªå°ç¢ç‰‡ã€‚
        """
        length = len(text)
        if length == 0:
            return []

            # åŸºç¡€å—å¤§å°
        chunk_size = max(1, length // num_chunks)
        chunks = []
        start = 0

        for i in range(num_chunks):
            if i == num_chunks - 1:
                # æœ€åä¸€ä¸ªå—ç›´æ¥å–åˆ°æœ«å°¾
                end = length
            else:
                target_end = start + chunk_size
                if target_end >= length:
                    end = length
                else:
                    # åœ¨ target_end ä¹‹åå¯»æ‰¾ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦
                    # find çš„ç¬¬äºŒä¸ªå‚æ•°æ˜¯èµ·å§‹æœç´¢ä½ç½®
                    newline_pos = text.find('\n', target_end)

                    if newline_pos != -1:
                        # åˆ‡åœ¨æ¢è¡Œç¬¦ä¹‹åï¼ŒåŒ…å«æ¢è¡Œç¬¦ (ä¿ç•™æ ¼å¼)
                        end = newline_pos + 1
                    else:
                        # å¦‚æœåé¢æ²¡æœ‰æ¢è¡Œç¬¦äº†ï¼Œå°±ä¸å¾—ä¸å¼ºåˆ¶åˆ‡åˆ†
                        # æˆ–è€…ä¸ºäº†å®‰å…¨ï¼Œç›´æ¥è¯»åˆ°æœ«å°¾ (å–å†³äºæ•°æ®åˆ†å¸ƒï¼Œé€šå¸¸ä¸å»ºè®®å¼ºåˆ‡)
                        end = length

            # æ·»åŠ åˆ‡ç‰‡
            if start < end:
                chunks.append(text[start:end])

            start = end
            if start >= length:
                break

        return chunks

        # -------------------------------------------------------------------------
        # æ›´æ–°åçš„ encode_parallel
        # -------------------------------------------------------------------------

    def encode_parallel(self, input_data: Union[str, List[str]], num_processes: int = None) -> List[int]:
        """
                å¹¶è¡Œç¼–ç æ¥å£ã€‚

                ä¼˜åŒ–ç‚¹ï¼š
                1. å¦‚æœè¾“å…¥æ˜¯ strï¼Œä½¿ç”¨æ™ºèƒ½åˆ†å—ç­–ç•¥ï¼ˆä¿ç•™æ¢è¡Œç¬¦ï¼Œæ§åˆ¶ä»»åŠ¡æ•°é‡ï¼‰ã€‚
                2. ä»»åŠ¡æ•°é‡æ§åˆ¶åœ¨ cpu_count * 4 å·¦å³ï¼Œæœ€å¤§åŒ–ååé‡ã€‚
                """
        print(f"Debug: Current cache size: {len(self.cache)}")
        original_cache = self.cache
        self.cache = {}
        if num_processes is None:
            num_processes = min(16, multiprocessing.cpu_count())

            # --- 1. ç±»å‹å…¼å®¹æ€§ä¸åˆ†å—ç­–ç•¥ä¼˜åŒ– ---
        if isinstance(input_data, str) and os.path.exists(input_data):
            file_path = input_data
            file_size = os.path.getsize(file_path)
            print(f"ğŸš€ Detected file input: {file_path} ({file_size / (1024 ** 3):.2f} GB)")

            # 1. è°ƒç”¨ä½ æä¾›çš„è¾¹ç•Œè®¡ç®—å‡½æ•° (å®Œç¾å¤ç”¨)
            # æ³¨æ„ï¼šè¦ä¼ å…¥ self.special_tokens ä»¥é˜²æ­¢åˆ‡æ–­ç‰¹æ®Š token
            target_chunks = num_processes * 4
            print(" -> Calculating chunk boundaries (mmap)...")
            boundaries = _get_chunk_boundaries(file_path, target_chunks, self.special_tokens)

            # 2. æ„é€  Worker å‚æ•°: (path, start, end)
            worker_args = [(file_path, start, end) for start, end in boundaries]

            # 3. æŒ‡å®š Worker å‡½æ•°
            target_worker = _worker_encode_from_file
            total_work_units = file_size  # è¿›åº¦æ¡æ€»é‡

        # =====================================================================
        # åˆ†æ”¯ B: è¾“å…¥æ˜¯å†…å­˜å­—ç¬¦ä¸²åˆ—è¡¨ -> èµ°æ—§é€»è¾‘
        # =====================================================================
        else:
            print("ğŸš€ Detected memory input (List[str]).")
            texts = input_data
            if isinstance(texts, str):  # å•ä¸ªå¤§å­—ç¬¦ä¸²å…œåº•
                texts = self._chunk_string(texts, num_processes * 4)

            worker_args = texts
            target_worker = _worker_encode  # åŸæ¥çš„ worker
            total_work_units = sum(len(t.encode('utf-8')) for t in worker_args)  # ä¼°ç®—å¤§å°

        print(f" -> Starting multiprocessing pool with {num_processes} workers...")
        # 2. åºåˆ—åŒ–å¿…è¦æ•°æ® (Lightweight Serialization)
        # vocab: è½¬ä¸º {int: list[int]}ï¼Œä½“ç§¯æ›´å°ä¸”å…¼å®¹ pickle
        serialized_vocab = self.vocab
        # merges: é‡å»ºä¸ºæœ‰åºåˆ—è¡¨ï¼Œå› ä¸º ranks å­—å…¸æ˜¯æ— åºçš„
        merges_list = [None] * len(self.ranks)  # åå‘æŸ¥ ï¼Œä½¿ç”¨æ•°å­—ç´¢å¼•ï¼Œå¼€é”€å°ã€‚
        for pair, rank in self.ranks.items():
            merges_list[rank] = pair
        # 3. å¯åŠ¨è¿›ç¨‹æ± 
        start_time = time.time()
        final_ids = []
        total_tokens = 0
        with multiprocessing.Pool(
                processes=num_processes,
                initializer=_worker_initializer,
                initargs=(serialized_vocab, self.original_merges, self.special_tokens)
        ) as pool:

            # ä½¿ç”¨ imap
            cursor = pool.imap(target_worker, worker_args, chunksize=1)

            with tqdm.tqdm(total=total_work_units, unit='B', unit_scale=True, desc="Tokenizing") as pbar:
                for chunk_ids, chunk_len in cursor:  # æ¥æ”¶ (ids, byte_len)
                    final_ids.extend(chunk_ids)

                    total_tokens += len(chunk_ids)
                    pbar.update(chunk_len)

                    # ä½ çš„é€Ÿåº¦æ˜¾ç¤ºé€»è¾‘ (ä¿æŒåŸæ ·)
                    elapsed = time.time() - start_time
                    if elapsed > 0:
                        # è®¡ç®—åŸå§‹é€Ÿåº¦
                        tokens_per_sec = total_tokens / elapsed

                        # æ ¼å¼åŒ–æ˜¾ç¤º (M tok/s æˆ– k tok/s)
                        if tokens_per_sec > 1_000_000:
                            speed_str = f"{tokens_per_sec / 1_000:.2f}k tok/s"
                        else:
                            speed_str = f"{tokens_per_sec / 1:.2f} tok/s"

                        pbar.set_postfix(
                            speed=speed_str,
                            # ratio=f"{bytes_per_token:.2f} bytes/tok" # å¯é€‰
                        )

        return final_ids























