import os
import argparse
import yaml
import torch
import math
import glob
import re
import wandb
from tqdm import tqdm

# å¼•å…¥ä½ çš„æ¨¡å—
from cs336_basics.model import TransformerLM
from cs336_basics.data import TextDataLoader
from cs336_basics.utils import cross_entropy

def extract_step(filename):
    """ä»æ–‡ä»¶å ckpt_2000.pt ä¸­æå–å‡ºæ•°å­—"""
    match = re.search(r'ckpt_(\d+)\.pt', filename)
    return int(match.group(1)) if match else -1

def evaluate_checkpoints(exp_config: dict, common_config: dict, project_name: str):
    run_name = exp_config['run_name']
    
    # 1. åˆå§‹åŒ– WandB
    wandb.init(
        project=project_name,
        name=f"{run_name}-eval",
        group=run_name,
        job_type="evaluation",
        config={"experiment": exp_config, "common": common_config}
    )

    device = common_config['training']['device']
    if device == 'cuda' and not torch.cuda.is_available():
        device = 'cpu'
    
    print(f"\n[{run_name}] å¼€å§‹è¯„ä¼° (Custom Mapping Mode)...")

    # 2. å‡†å¤‡æ•°æ®
    val_path = common_config['data'].get('val_path')
    if not val_path:
        print(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'val_path'")
        wandb.finish()
        return

    val_loader = TextDataLoader(
        input_data=val_path,
        batch_size=common_config['training']['batch_size'],
        context_length=exp_config['model']['context_length'],
        device=device
    )

    # 3. åˆå§‹åŒ–æ¨¡å‹
    model_config = exp_config['model']
    model = TransformerLM(
        vocab_size=model_config['vocab_size'],
        context_length=model_config['context_length'],
        d_model=model_config['d_model'],
        num_layers=model_config['num_layers'],
        num_heads=model_config['num_heads'],
        d_ff=model_config['d_ff'],
        rope_theta=model_config.get('rope_theta', 10000.0),
        config=model_config,
        device=device,
        dtype=torch.float32
    )
    model.to(device)
    model.eval()

    # 4. æŸ¥æ‰¾æ–‡ä»¶
    out_dir = os.path.join(common_config['checkpoint']['out_dir_base'], run_name)
    if not os.path.exists(out_dir):
        print(f"âŒ æ‰¾ä¸åˆ°ç›®å½•: {out_dir}")
        wandb.finish()
        return

    ckpt_files = glob.glob(os.path.join(out_dir, "ckpt_*.pt"))
    ckpt_files.sort(key=extract_step) # æŒ‰æ­¥æ•°æ’åº

    print(f"ğŸ“‚ æ‰¾åˆ° {len(ckpt_files)} ä¸ª checkpoint æ–‡ä»¶ã€‚")

    # 5. å¾ªç¯è¯„ä¼°
    eval_iters = 200 

    for ckpt_path in tqdm(ckpt_files, desc=f"Evaluating {run_name}"):
        file_step = extract_step(ckpt_path)
        
        # === ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šè‡ªå®šä¹‰æ˜ å°„é€»è¾‘ ===
        if file_step == 25000:
            # è¿™é‡Œçš„éœ€æ±‚æ˜¯ï¼šæŠŠ 25000 å¼ºè¡Œç”»åœ¨ 24000 çš„ä½ç½®ä¸Š
            wandb_step = 24000
        else:
            # å…¶ä»–çš„æŒ‰åŸè®¡åˆ’ï¼šå‘å‰å¹³ç§» 2000 (ä¾‹å¦‚ 2000->0, 4000->2000)
            wandb_step = file_step - 2000
        
        # å¦‚æœæ˜¯ ckpt_0.pt (file_step=0 -> wandb_step=-2000)ï¼Œè·³è¿‡
        if wandb_step < 0:
            continue
            
        # å¦‚æœä¸éœ€è¦è¶…è¿‡ 24000 çš„ç‚¹ï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæˆªæ–­
        # if wandb_step > 24000: break

        try:
            # åŠ è½½æƒé‡ (ä¿®å¤äº† missing key é—®é¢˜)
            checkpoint = torch.load(ckpt_path, map_location=device)
            
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            elif isinstance(checkpoint, dict) and 'model' in checkpoint:
                model.load_state_dict(checkpoint['model'])
            else:
                model.load_state_dict(checkpoint)
                
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ {ckpt_path}: {e}")
            continue

        # è®¡ç®— Validation Loss
        losses = []
        with torch.no_grad():
            for _ in range(eval_iters):
                x, y = val_loader.get_batch()
                logits = model(x)
                loss = cross_entropy(logits, y)
                losses.append(loss.item())

        mean_loss = sum(losses) / len(losses)
        perplexity = math.exp(mean_loss)

        # è®°å½•åˆ° WandB
        wandb.log({
            "val/loss": mean_loss,
            "val/ppl": perplexity,
            "source_ckpt": file_step  # è®°å½•ä¸€ä¸‹åŸå§‹æ˜¯å“ªä¸ªæ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥éªŒ
        }, step=wandb_step)

    wandb.finish()
    print(f"âœ… [{run_name}] è¯„ä¼°å®Œæˆï¼")


def run_wandb_evaluation(config_path):
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    project_name = config.get('project_name', 'cs336-transformer-ablation')
    
    # WandB Key
    my_wandb_key = "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"
    try:
        wandb.login(key=my_wandb_key)
    except:
        pass

    print(f"=== æ­£åœ¨å¤„ç†é…ç½®æ–‡ä»¶: {config_path} ===")
    
    for exp in config['experiments']:
        evaluate_checkpoints(exp, config['common'], project_name)

if __name__ == "__main__":
    my_wandb_key = "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"
    if my_wandb_key == "è¿™é‡Œç²˜è´´ä½ çš„API_KEY":
        print("è­¦å‘Š: ä½ è¿˜æ²¡æœ‰è®¾ç½® API Keyã€‚å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œè¯·åœ¨ä»£ç æœ€åä¸€è¡Œå¡«å…¥ keyï¼Œæˆ–è€…æ‰‹åŠ¨è¿è¡Œ wandb loginã€‚")
    else:
        wandb.login(key=my_wandb_key)
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, required=True, help='Path to yaml config file')
    args = parser.parse_args()

    run_wandb_evaluation(args.config)