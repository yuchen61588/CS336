#### 一个batch的结构

一个batch绝对不是只有一个训练数据，而是每个训练数据都有一个target数据进行配对

在训练循环中，数据加载器吐出的每一个 Batch 实际上是一个**元组（Tuple）**，里面装了两个张量（Tensor）：`Input` 和 `Target`。

**Inputs (输入/题目)**: 喂给模型的，让模型根据这些去猜下一个字。

**Targets (目标/答案)**: 用来告诉模型“刚才你猜对了没”，用于计算 Loss（误差）。

对于inputs和targets,一般来说都是targets比inputs快一个字母。

##### 2. 微观样子：二维矩阵

假设我们设定的参数是：

- **Batch Size ($B$) = 4**: 一次并行处理 4 条句子。
- **Context Length ($m$ 或 $T$) = 8**: 每条句子看 8 个字。

那么这个 Batch 里的两个张量长这样：

##### **张量 A：Inputs (输入)**

形状：`(B, T)` 即 `(4, 8)`  这是一个整数矩阵，里面全是 Token ID。



| **句子索引** | **第1个字** | **第2个字** | **第3个字** | **...**  | **第8个字** |
| ------------ | ----------- | ----------- | ----------- | -------- | ----------- |
| **句 1**     | Once        | upon        | a           | time     | ...         |
| **句 2**     | The         | cat         | sat         | on       | ...         |
| **句 3**     | She         | said        | hello       | to       | ...         |
| **句 4**     | I           | love        | machine     | learning | ...         |

##### **张量 B：Targets (答案)**

形状：`(B, T)` 即 `(4, 8)`  **注意：** 它和上面的 Input 内容几乎一样，只是整体**向左移动了一位**（也就是 Input 的下一个字）。



| **句子索引** | **第1个字** | **第2个字** | **第3个字** | **...** | **第8个字** |
| ------------ | ----------- | ----------- | ----------- | ------- | ----------- |
| **句 1**     | upon        | a           | time        | there   | ...         |
| **句 2**     | cat         | sat         | on          | the     | ...         |
| **句 3**     | said        | hello       | to          | the     | ...         |
| **句 4**     | love        | machine     | learning    | very    | ...         |

#### 要求

**交付物：** 编写一个函数，该函数接收以下参数：

- 一个 numpy 数组 `x`（包含令牌 ID 的整数数组）。
- `batch_size`（批次大小）。
- `context_length`（上下文长度，即序列长度）。
- 一个 PyTorch 设备字符串（例如 `'cpu'` 或 `'cuda:0'`）。

函数应返回一对张量（tensors）：采样得到的**输入序列**和对应的**下一个令牌目标**。两个张量的形状都应为 `(batch_size, context_length)`，包含令牌 ID，并且都应放置在请求的设备上。

为了测试你的实现，你需要先实现 `[adapters.run_get_batch]` 中的测试适配器。然后运行 `uv run pytest -k test_get_batch` 来测试你的实现。



#### 断点存储设置

运行任务时，我们经常希望能够恢复因某种原因（如超时、机器故障等）中途停止的训练。即使一切顺利，我们可能也希望以后能访问中间模型（例如，事后分析训练动态，从不同训练阶段的模型中采样等）。

一个检查点应包含我们恢复训练所需的所有状态。

**模型权重**：最基础的 (`model.state_dict()`)。

**优化器状态**：如果使用有状态的优化器（如 AdamW 记录了动量），必须保存 (`optimizer.state_dict()`)，否则恢复训练后梯度更新会不正常。

**迭代次数**：为了恢复学习率调度（Learning Rate Schedule），我们需要知道当前训练到了第几步。

 PyTorch 的 `torch.save` 可以将一个对象（通常是一个字典）保存到文件。

