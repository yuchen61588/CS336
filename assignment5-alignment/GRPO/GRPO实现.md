### 一些需要注意的地方

1. **损失函数**

   **其次，我们将引入一个新的基准（Benchmark）来评估模型** 。然而，本次作业的重点是弥合基础模型与下游任务之间的差距（后训练），因此我们必须使用独立于交叉熵的评估方法 。我们将使用来自 Hendrycks 等人（2021）的 **MATH 12K 数据集**，该数据集由极具挑战性的高中数学竞赛题目组成 。我们将通过将模型输出与参考答案进行对比来评估模型 。

   **评估范式的转变**：不再仅仅通过预测下一个 token 的准确性（交叉熵损失）来衡量模型好坏，而是通过**最终答案的正确性**来衡量 。这反映了业界从关注“模型学得像不像”到关注“模型能不能解决问题”的转变。

2. **思维链**

​       思维链是指在得出最终答案之前，逐步思考问题并生成中间推理步骤的过程 。

**大语言模型的思维链推理**：早期的思维链方法通过使用“草稿本”（scratchpad）将问题拆解为中间步骤，从而微调模型解决简单的算术任务 。其他研究通过提示（Prompt）让强大的模型在回答前“一步步思考”，发现这显著提高了在年级数学题等推理任务上的表现 。

**通过专家迭代（Expert Iteration）学习推理**：自我提升推理者（STaR）框架将推理视为一个引导循环：预训练模型首先采样多种多样的思维链，仅保留那些导致正确答案的样本，然后对这些“专家”轨迹进行微调 。迭代这一循环可以提高模型的推理能力和解题率 。STaR 证明了这种使用基于字符串匹配的自动验证方法，可以在没有人类编写推理轨迹的情况下引导出推理技能 。

**通过验证奖励、o1 和 R1 进行推理强化学习**：最近的研究探索了使用更强大的强化学习算法和验证奖励来提高推理性能 。OpenAI 的 **o1**、DeepSeek 的 **R1** 以及 Moonshot 的 **Kimi k1.5** 使用策略梯度方法，在数学和代码任务上进行训练，通过字符串匹配或单元测试验证正确性，在竞赛数学和编程表现上取得了显著进步 。随后的工作证实，即使是在 1.5B 参数的小模型上，纯粹的验证奖励强化学习也能提升推理表现 。

