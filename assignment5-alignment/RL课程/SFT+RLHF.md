# 实现对语言模型（LM）输出更好、更严密的控制

预训练数据并不完全是我们想要的（但它具有可扩展性）……

> **我们能否收集我们“确实想要”的行为数据并训练语言模型？**

1. **那些数据看起来是什么样的？**
2. **我们如何才能最好地利用这些数据？**
3. **我们为此需要规模（Scale）吗？**

## **标准方法 —— 模仿（SFT）随后进行强化学习（‘RL’HF）**

#### **第 1 步：收集演示数据并训练有监督策略（SFT）**

- 从我们的提示词（Prompt）数据集中采样一个提示。
- 标注员演示理想的输出行为。
- 这些数据被用于通过有监督学习（Supervised Learning）对 GPT-3 进行微调。

#### **第 2 步：收集比较数据并训练奖励模型（RM）**

- 采样一个提示和多个模型输出结果。
- 标注员将这些输出结果按质量从最好到最差进行排序。
- 这些数据被用于训练我们的奖励模型（Reward Model）。

#### **第 3 步：使用强化学习针对奖励模型优化策略（RL）**

- 从数据集中采样一个新提示。
- 策略（模型）生成一个输出。
- 奖励模型计算该输出的奖励值。
- 该奖励被用于通过 PPO 算法更新策略。